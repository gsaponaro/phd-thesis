%!TEX encoding = UTF-8

Personal and service robots may benefit society in different activities and challenges, thanks to their increasingly advanced mechanical and decision-making capabilities.
However, for that to happen one of the essential conditions is to have coherent, natural and intuitive interfaces so that humans can make a fruitful and effective use of these intelligent machines.
In general, the goal of reaching intuitive interfaces for \hri{} has not yet been attained, partly due to the fact that robots are not yet widespread in public spaces, partly due to the technical difficulties in interpreting human intentions.

Making service robots that understand their surroundings entails that they should possess capabilities that allow them to operate in unstructured environments and under unpredictable conditions, unlike industrial robots which usually operate in highly structured, controlled and repeatable environments.
To tackle these challenges, in this thesis we develop computational models based on findings from developmental psychology~(object affordances) and from neuroscience~(mirror neuron system).
The proposed models stem from the consideration that objects carry information about actions and interactions.

We contribute a modular framework for visual robot affordance learning, based on autonomous robot exploration of the world, sensorimotor data collection and statistical models.
We combine affordances with communication (gestures and language), to interpret and describe human actions in manipulative scenes by reusing previous robot experience.
We show a model that deals with multiple objects, giving rise to tool use, including the link from hand affordances~(i.e., action possibilities by using the hands) to tool affordances~(i.e., action possibilities by using tools).
We illustrate how affordances can be used for planning complex manipulation tasks under noise and uncertainty.
In two appendixes,
we describe a robot model for recognizing human gestures in manipulation scenarios,
and we report a study about how people perceive robot gestures when the facial information is turned off.

\bigskip

\textbf{Keywords}: \myThesisKeywords
